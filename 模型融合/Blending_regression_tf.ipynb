{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  target  \n",
       "0    -122.23   4.526  \n",
       "1    -122.22   3.585  \n",
       "2    -122.24   3.521  \n",
       "3    -122.25   3.413  \n",
       "4    -122.25   3.422  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df = pd.DataFrame(data=housing['data'], columns=housing['feature_names'])\n",
    "df['target'] = housing['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 23:03:11.988063: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def smish(x):\n",
    "    return x * K.tanh(K.log(1 + K.sigmoid(x)))\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class GatedLinearUnit(L.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.linear = L.Dense(units)\n",
    "        self.sigmoid = L.Dense(units, activation=\"sigmoid\")\n",
    "        self.units = units\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs) * self.sigmoid(inputs)\n",
    "    \n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class GatedResidualNetwork(L.Layer):\n",
    "    def __init__(self, units, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.relu_dense = L.Dense(units, activation=smish)\n",
    "        self.linear_dense = L.Dense(units)\n",
    "        self.dropout = L.Dropout(dropout_rate)\n",
    "        self.gated_linear_unit = GatedLinearUnit(units)\n",
    "        self.layer_norm = L.LayerNormalization()\n",
    "        self.project = L.Dense(units)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['dropout_rate'] = self.dropout_rate\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.relu_dense(inputs)\n",
    "        x = self.linear_dense(x)\n",
    "        x = self.dropout(x)\n",
    "        if inputs.shape[-1] != self.units:\n",
    "            inputs = self.project(inputs)\n",
    "        x = inputs + self.gated_linear_unit(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class VariableSelection(L.Layer):\n",
    "    def __init__(self, num_features, units, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.grns = list()\n",
    "        # Create a GRN for each feature independently\n",
    "        for idx in range(num_features):\n",
    "            grn = GatedResidualNetwork(units, dropout_rate)\n",
    "            self.grns.append(grn)\n",
    "        # Create a GRN for the concatenation of all the features\n",
    "        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n",
    "        self.softmax = L.Dense(units=num_features, activation=\"softmax\")\n",
    "        self.num_features = num_features\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['num_features'] = self.num_features\n",
    "        config['units'] = self.units\n",
    "        config['dropout_rate'] = self.dropout_rate\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        v = L.concatenate(inputs)\n",
    "        v = self.grn_concat(v)\n",
    "        v = tf.expand_dims(self.softmax(v), axis=-1)\n",
    "\n",
    "        x = []\n",
    "        for idx, input_ in enumerate(inputs):\n",
    "            x.append(self.grns[idx](input_))\n",
    "        x = tf.stack(x, axis=1)\n",
    "\n",
    "        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class VariableSelectionFlow(L.Layer):\n",
    "    def __init__(self, num_features, units, dropout_rate, dense_units=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.variableselection = VariableSelection(num_features, units, dropout_rate)\n",
    "        self.split = L.Lambda(lambda t: tf.split(t, num_features, axis=-1))\n",
    "        self.dense = dense_units\n",
    "        if dense_units:\n",
    "            self.dense_list = [L.Dense(dense_units, \\\n",
    "                                       activation='linear') \\\n",
    "                               for _ in tf.range(num_features)\n",
    "                              ]\n",
    "        self.num_features = num_features\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dense_units = dense_units\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['num_features'] = self.num_features\n",
    "        config['units'] = self.units\n",
    "        config['dropout_rate'] = self.dropout_rate\n",
    "        config['dense_units'] = self.dense_units\n",
    "        return config        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        split_input = self.split(inputs)\n",
    "        if self.dense:\n",
    "            l = [self.dense_list[i](split_input[i]) for i in range(len(self.dense_list))]\n",
    "        else:\n",
    "            l = split_input\n",
    "        return self.variableselection(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(df.drop(columns=['target'], axis=1), df[['target']], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______fold 1______, ________repeat 1__________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13/13 [==============================] - 258s 1s/step - loss: 2.5957 - val_loss: 1.4050 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "13/13 [==============================] - 3s 254ms/step - loss: 1.3945 - val_loss: 1.3282 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3377\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "13/13 [==============================] - 3s 214ms/step - loss: 1.3377 - val_loss: 1.3472 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 1.3385\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "13/13 [==============================] - 3s 232ms/step - loss: 1.3385 - val_loss: 1.3356 - lr: 9.5000e-04\n",
      "Epoch 5/100\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 1.3360 - val_loss: 1.3266 - lr: 9.0250e-04\n",
      "Epoch 6/100\n",
      "13/13 [==============================] - 3s 226ms/step - loss: 1.3293 - val_loss: 1.3211 - lr: 9.0250e-04\n",
      "Epoch 7/100\n",
      "13/13 [==============================] - 3s 222ms/step - loss: 1.3210 - val_loss: 1.2945 - lr: 9.0250e-04\n",
      "Epoch 8/100\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 1.2883 - val_loss: 1.1278 - lr: 9.0250e-04\n",
      "Epoch 9/100\n",
      "13/13 [==============================] - 3s 233ms/step - loss: 1.1587 - val_loss: 0.9478 - lr: 9.0250e-04\n",
      "Epoch 10/100\n",
      "13/13 [==============================] - 3s 233ms/step - loss: 0.9975 - val_loss: 0.8674 - lr: 9.0250e-04\n",
      "Epoch 11/100\n",
      "13/13 [==============================] - 3s 237ms/step - loss: 0.9051 - val_loss: 0.8097 - lr: 9.0250e-04\n",
      "Epoch 12/100\n",
      "13/13 [==============================] - 3s 234ms/step - loss: 0.8408 - val_loss: 0.7280 - lr: 9.0250e-04\n",
      "Epoch 13/100\n",
      "13/13 [==============================] - 3s 227ms/step - loss: 0.7664 - val_loss: 0.6574 - lr: 9.0250e-04\n",
      "Epoch 14/100\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.7228 - val_loss: 0.6238 - lr: 9.0250e-04\n",
      "Epoch 15/100\n",
      "13/13 [==============================] - 3s 228ms/step - loss: 0.6860 - val_loss: 0.5867 - lr: 9.0250e-04\n",
      "Epoch 16/100\n",
      "13/13 [==============================] - 3s 222ms/step - loss: 0.6625 - val_loss: 0.5700 - lr: 9.0250e-04\n",
      "Epoch 17/100\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.6404 - val_loss: 0.5504 - lr: 9.0250e-04\n",
      "Epoch 18/100\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.6357 - val_loss: 0.5421 - lr: 9.0250e-04\n",
      "Epoch 19/100\n",
      "13/13 [==============================] - 3s 220ms/step - loss: 0.6073 - val_loss: 0.5148 - lr: 9.0250e-04\n",
      "Epoch 20/100\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.6043 - val_loss: 0.5081 - lr: 9.0250e-04\n",
      "Epoch 21/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5941\n",
      "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "13/13 [==============================] - 3s 199ms/step - loss: 0.5941 - val_loss: 0.5312 - lr: 9.0250e-04\n",
      "Epoch 22/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5781\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 0.0008145062311086804.\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.5781 - val_loss: 0.5187 - lr: 8.5737e-04\n",
      "Epoch 23/100\n",
      "13/13 [==============================] - 3s 217ms/step - loss: 0.5668 - val_loss: 0.4879 - lr: 8.1451e-04\n",
      "Epoch 24/100\n",
      "13/13 [==============================] - 5s 398ms/step - loss: 0.5544 - val_loss: 0.4857 - lr: 8.1451e-04\n",
      "Epoch 25/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5474\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.0007737808919046074.\n",
      "13/13 [==============================] - 3s 206ms/step - loss: 0.5474 - val_loss: 0.4881 - lr: 8.1451e-04\n",
      "Epoch 26/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5422\n",
      "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.000735091819660738.\n",
      "13/13 [==============================] - 3s 221ms/step - loss: 0.5422 - val_loss: 0.4964 - lr: 7.7378e-04\n",
      "Epoch 27/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5342\n",
      "Epoch 27: ReduceLROnPlateau reducing learning rate to 0.0006983372120885178.\n",
      "13/13 [==============================] - 3s 215ms/step - loss: 0.5342 - val_loss: 0.4901 - lr: 7.3509e-04\n",
      "Epoch 28/100\n",
      "13/13 [==============================] - 3s 229ms/step - loss: 0.5270 - val_loss: 0.4723 - lr: 6.9834e-04\n",
      "Epoch 29/100\n",
      "13/13 [==============================] - 4s 351ms/step - loss: 0.5288 - val_loss: 0.4639 - lr: 6.9834e-04\n",
      "Epoch 30/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5228\n",
      "Epoch 30: ReduceLROnPlateau reducing learning rate to 0.0006634203542489559.\n",
      "13/13 [==============================] - 4s 313ms/step - loss: 0.5228 - val_loss: 0.4793 - lr: 6.9834e-04\n",
      "Epoch 31/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5207\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0006302493420662358.\n",
      "13/13 [==============================] - 4s 318ms/step - loss: 0.5207 - val_loss: 0.4961 - lr: 6.6342e-04\n",
      "Epoch 32/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5206\n",
      "Epoch 32: ReduceLROnPlateau reducing learning rate to 0.0005987368611386045.\n",
      "13/13 [==============================] - 3s 265ms/step - loss: 0.5206 - val_loss: 0.4784 - lr: 6.3025e-04\n",
      "Epoch 33/100\n",
      "13/13 [==============================] - 4s 293ms/step - loss: 0.5135 - val_loss: 0.4565 - lr: 5.9874e-04\n",
      "Epoch 34/100\n",
      "13/13 [==============================] - 3s 211ms/step - loss: 0.5183 - val_loss: 0.4552 - lr: 5.9874e-04\n",
      "Epoch 35/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5230\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0005688000208465382.\n",
      "13/13 [==============================] - 2s 155ms/step - loss: 0.5230 - val_loss: 0.4718 - lr: 5.9874e-04\n",
      "Epoch 36/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5107\n",
      "Epoch 36: ReduceLROnPlateau reducing learning rate to 0.0005403600225690752.\n",
      "13/13 [==============================] - 2s 170ms/step - loss: 0.5107 - val_loss: 0.4711 - lr: 5.6880e-04\n",
      "Epoch 37/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5056\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 0.0005133419937919825.\n",
      "13/13 [==============================] - 2s 188ms/step - loss: 0.5056 - val_loss: 0.4584 - lr: 5.4036e-04\n",
      "Epoch 38/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5084\n",
      "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0004876748775132.\n",
      "13/13 [==============================] - 3s 238ms/step - loss: 0.5084 - val_loss: 0.4755 - lr: 5.1334e-04\n",
      "Epoch 39/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5063\n",
      "Epoch 39: ReduceLROnPlateau reducing learning rate to 0.00046329112810781223.\n",
      "13/13 [==============================] - 3s 213ms/step - loss: 0.5063 - val_loss: 0.4708 - lr: 4.8767e-04\n",
      "Epoch 40/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5007\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.00044012657308485355.\n",
      "13/13 [==============================] - 3s 276ms/step - loss: 0.5007 - val_loss: 0.4774 - lr: 4.6329e-04\n",
      "Epoch 41/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5037\n",
      "Epoch 41: ReduceLROnPlateau reducing learning rate to 0.00041812024719547477.\n",
      "13/13 [==============================] - 4s 303ms/step - loss: 0.5037 - val_loss: 0.4633 - lr: 4.4013e-04\n",
      "Epoch 42/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5035\n",
      "Epoch 42: ReduceLROnPlateau reducing learning rate to 0.00039721422654110934.\n",
      "13/13 [==============================] - 4s 329ms/step - loss: 0.5035 - val_loss: 0.4644 - lr: 4.1812e-04\n",
      "Epoch 43/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4975\n",
      "Epoch 43: ReduceLROnPlateau reducing learning rate to 0.00037735351797891776.\n",
      "13/13 [==============================] - 2s 181ms/step - loss: 0.4975 - val_loss: 0.4594 - lr: 3.9721e-04\n",
      "Epoch 44/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5003\n",
      "Epoch 44: ReduceLROnPlateau reducing learning rate to 0.00035848583793267607.\n",
      "13/13 [==============================] - 3s 242ms/step - loss: 0.5003 - val_loss: 0.4643 - lr: 3.7735e-04\n",
      "Epoch 45/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.5027\n",
      "Epoch 45: ReduceLROnPlateau reducing learning rate to 0.00034056155709549785.\n",
      "13/13 [==============================] - 4s 322ms/step - loss: 0.5027 - val_loss: 0.4591 - lr: 3.5849e-04\n",
      "Epoch 46/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4999\n",
      "Epoch 46: ReduceLROnPlateau reducing learning rate to 0.00032353347924072293.\n",
      "13/13 [==============================] - 3s 199ms/step - loss: 0.4999 - val_loss: 0.4699 - lr: 3.4056e-04\n",
      "Epoch 47/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4957\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.00030735681357327847.\n",
      "13/13 [==============================] - 2s 188ms/step - loss: 0.4957 - val_loss: 0.4581 - lr: 3.2353e-04\n",
      "Epoch 48/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4947\n",
      "Epoch 48: ReduceLROnPlateau reducing learning rate to 0.00029198898118920624.\n",
      "13/13 [==============================] - 3s 212ms/step - loss: 0.4947 - val_loss: 0.4608 - lr: 3.0736e-04\n",
      "Epoch 49/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4964\n",
      "Epoch 49: ReduceLROnPlateau reducing learning rate to 0.00027738953212974593.\n",
      "13/13 [==============================] - 2s 187ms/step - loss: 0.4964 - val_loss: 0.4821 - lr: 2.9199e-04\n",
      "Epoch 50/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4985\n",
      "Epoch 50: ReduceLROnPlateau reducing learning rate to 0.0002635200624354184.\n",
      "13/13 [==============================] - 4s 310ms/step - loss: 0.4985 - val_loss: 0.4730 - lr: 2.7739e-04\n",
      "Epoch 51/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4975\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.0002503440482541919.\n",
      "13/13 [==============================] - 3s 239ms/step - loss: 0.4975 - val_loss: 0.4706 - lr: 2.6352e-04\n",
      "Epoch 52/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4951\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.00023782684584148226.\n",
      "13/13 [==============================] - 2s 174ms/step - loss: 0.4951 - val_loss: 0.4617 - lr: 2.5034e-04\n",
      "Epoch 53/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4922\n",
      "Epoch 53: ReduceLROnPlateau reducing learning rate to 0.00022593549801968037.\n",
      "13/13 [==============================] - 2s 148ms/step - loss: 0.4922 - val_loss: 0.4672 - lr: 2.3783e-04\n",
      "Epoch 54/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4944\n",
      "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.00021463872035383245.\n",
      "13/13 [==============================] - 5s 428ms/step - loss: 0.4944 - val_loss: 0.4791 - lr: 2.2594e-04\n",
      "Epoch 55/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4942\n",
      "Epoch 55: ReduceLROnPlateau reducing learning rate to 0.0002039067905570846.\n",
      "13/13 [==============================] - 6s 410ms/step - loss: 0.4942 - val_loss: 0.4631 - lr: 2.1464e-04\n",
      "Epoch 56/100\n",
      "13/13 [==============================] - 9s 660ms/step - loss: 0.4939 - val_loss: 0.4547 - lr: 2.0391e-04\n",
      "Epoch 57/100\n",
      "13/13 [==============================] - 5s 398ms/step - loss: 0.4909 - val_loss: 0.4538 - lr: 2.0391e-04\n",
      "Epoch 58/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4952\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.00019371145172044634.\n",
      "13/13 [==============================] - 4s 340ms/step - loss: 0.4952 - val_loss: 0.4596 - lr: 2.0391e-04\n",
      "Epoch 59/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4918\n",
      "Epoch 59: ReduceLROnPlateau reducing learning rate to 0.00018402588466415182.\n",
      "13/13 [==============================] - 2s 193ms/step - loss: 0.4918 - val_loss: 0.4733 - lr: 1.9371e-04\n",
      "Epoch 60/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4938\n",
      "Epoch 60: ReduceLROnPlateau reducing learning rate to 0.00017482458351878447.\n",
      "13/13 [==============================] - 3s 201ms/step - loss: 0.4938 - val_loss: 0.4788 - lr: 1.8403e-04\n",
      "Epoch 61/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4909\n",
      "Epoch 61: ReduceLROnPlateau reducing learning rate to 0.0001660833557252772.\n",
      "13/13 [==============================] - 2s 158ms/step - loss: 0.4909 - val_loss: 0.4816 - lr: 1.7482e-04\n",
      "Epoch 62/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4893\n",
      "Epoch 62: ReduceLROnPlateau reducing learning rate to 0.0001577791837917175.\n",
      "13/13 [==============================] - 2s 143ms/step - loss: 0.4893 - val_loss: 0.4559 - lr: 1.6608e-04\n",
      "Epoch 63/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4894\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.0001498902252933476.\n",
      "13/13 [==============================] - 2s 156ms/step - loss: 0.4894 - val_loss: 0.4631 - lr: 1.5778e-04\n",
      "Epoch 64/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4902\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.00014239571610232815.\n",
      "13/13 [==============================] - 2s 179ms/step - loss: 0.4902 - val_loss: 0.4556 - lr: 1.4989e-04\n",
      "Epoch 65/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4862\n",
      "Epoch 65: ReduceLROnPlateau reducing learning rate to 0.00013527592891477978.\n",
      "13/13 [==============================] - 2s 155ms/step - loss: 0.4862 - val_loss: 0.4614 - lr: 1.4240e-04\n",
      "Epoch 66/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4890\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.00012851213177782482.\n",
      "13/13 [==============================] - 2s 162ms/step - loss: 0.4890 - val_loss: 0.4571 - lr: 1.3528e-04\n",
      "Epoch 67/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4903\n",
      "Epoch 67: ReduceLROnPlateau reducing learning rate to 0.00012208651896798982.\n",
      "13/13 [==============================] - 2s 153ms/step - loss: 0.4903 - val_loss: 0.4595 - lr: 1.2851e-04\n",
      "Epoch 68/100\n",
      "13/13 [==============================] - 2s 167ms/step - loss: 0.4878 - val_loss: 0.4467 - lr: 1.2209e-04\n",
      "Epoch 69/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4889\n",
      "Epoch 69: ReduceLROnPlateau reducing learning rate to 0.00011598219716688617.\n",
      "13/13 [==============================] - 2s 157ms/step - loss: 0.4889 - val_loss: 0.4512 - lr: 1.2209e-04\n",
      "Epoch 70/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4885\n",
      "Epoch 70: ReduceLROnPlateau reducing learning rate to 0.00011018308869097381.\n",
      "13/13 [==============================] - 2s 157ms/step - loss: 0.4885 - val_loss: 0.4572 - lr: 1.1598e-04\n",
      "Epoch 71/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4868\n",
      "Epoch 71: ReduceLROnPlateau reducing learning rate to 0.00010467393149156123.\n",
      "13/13 [==============================] - 2s 159ms/step - loss: 0.4868 - val_loss: 0.4624 - lr: 1.1018e-04\n",
      "Epoch 72/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4862\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 9.944023768184706e-05.\n",
      "13/13 [==============================] - 2s 180ms/step - loss: 0.4862 - val_loss: 0.4685 - lr: 1.0467e-04\n",
      "Epoch 73/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4870\n",
      "Epoch 73: ReduceLROnPlateau reducing learning rate to 9.446822441532275e-05.\n",
      "13/13 [==============================] - 2s 148ms/step - loss: 0.4870 - val_loss: 0.4597 - lr: 9.9440e-05\n",
      "Epoch 74/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4877\n",
      "Epoch 74: ReduceLROnPlateau reducing learning rate to 8.974481388577259e-05.\n",
      "13/13 [==============================] - 2s 141ms/step - loss: 0.4877 - val_loss: 0.4549 - lr: 9.4468e-05\n",
      "Epoch 75/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4908\n",
      "Epoch 75: ReduceLROnPlateau reducing learning rate to 8.525757111783605e-05.\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.4908 - val_loss: 0.4494 - lr: 8.9745e-05\n",
      "Epoch 76/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4879\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 8.099469014268834e-05.\n",
      "13/13 [==============================] - 2s 148ms/step - loss: 0.4879 - val_loss: 0.4531 - lr: 8.5258e-05\n",
      "Epoch 77/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4875\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 7.694495252508204e-05.\n",
      "13/13 [==============================] - 2s 157ms/step - loss: 0.4875 - val_loss: 0.4552 - lr: 8.0995e-05\n",
      "Epoch 78/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4862\n",
      "Epoch 78: ReduceLROnPlateau reducing learning rate to 7.309770662686787e-05.\n",
      "13/13 [==============================] - 2s 157ms/step - loss: 0.4862 - val_loss: 0.4516 - lr: 7.6945e-05\n",
      "Epoch 79/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4899\n",
      "Epoch 79: ReduceLROnPlateau reducing learning rate to 6.944281922187656e-05.\n",
      "13/13 [==============================] - 2s 148ms/step - loss: 0.4899 - val_loss: 0.4632 - lr: 7.3098e-05\n",
      "Epoch 80/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4853\n",
      "Epoch 80: ReduceLROnPlateau reducing learning rate to 6.597067549591884e-05.\n",
      "13/13 [==============================] - 2s 151ms/step - loss: 0.4853 - val_loss: 0.4546 - lr: 6.9443e-05\n",
      "Epoch 81/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4894\n",
      "Epoch 81: ReduceLROnPlateau reducing learning rate to 6.267214448598679e-05.\n",
      "13/13 [==============================] - 2s 148ms/step - loss: 0.4894 - val_loss: 0.4529 - lr: 6.5971e-05\n",
      "Epoch 82/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4865\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 5.953853760729544e-05.\n",
      "13/13 [==============================] - 2s 144ms/step - loss: 0.4865 - val_loss: 0.4572 - lr: 6.2672e-05\n",
      "Epoch 83/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4855\n",
      "Epoch 83: ReduceLROnPlateau reducing learning rate to 5.656161210936261e-05.\n",
      "13/13 [==============================] - 2s 154ms/step - loss: 0.4855 - val_loss: 0.4580 - lr: 5.9539e-05\n",
      "Epoch 84/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4876\n",
      "Epoch 84: ReduceLROnPlateau reducing learning rate to 5.373353305913042e-05.\n",
      "13/13 [==============================] - 2s 165ms/step - loss: 0.4876 - val_loss: 0.4542 - lr: 5.6562e-05\n",
      "Epoch 85/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4861\n",
      "Epoch 85: ReduceLROnPlateau reducing learning rate to 5.104685606056591e-05.\n",
      "13/13 [==============================] - 2s 150ms/step - loss: 0.4861 - val_loss: 0.4572 - lr: 5.3734e-05\n",
      "Epoch 86/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4857\n",
      "Epoch 86: ReduceLROnPlateau reducing learning rate to 4.849451343034161e-05.\n",
      "13/13 [==============================] - 2s 146ms/step - loss: 0.4857 - val_loss: 0.4536 - lr: 5.1047e-05\n",
      "Epoch 87/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4843\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 4.606978654919658e-05.\n",
      "13/13 [==============================] - 2s 145ms/step - loss: 0.4843 - val_loss: 0.4574 - lr: 4.8495e-05\n",
      "Epoch 88/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4880\n",
      "Epoch 88: ReduceLROnPlateau reducing learning rate to 4.3766295493696814e-05.\n",
      "13/13 [==============================] - 2s 147ms/step - loss: 0.4880 - val_loss: 0.4539 - lr: 4.6070e-05\n",
      "Epoch 89/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4878\n",
      "Epoch 89: ReduceLROnPlateau reducing learning rate to 4.1577981755835934e-05.\n",
      "13/13 [==============================] - 2s 158ms/step - loss: 0.4878 - val_loss: 0.4584 - lr: 4.3766e-05\n",
      "Epoch 90/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4852\n",
      "Epoch 90: ReduceLROnPlateau reducing learning rate to 3.949908405047608e-05.\n",
      "13/13 [==============================] - 3s 225ms/step - loss: 0.4852 - val_loss: 0.4513 - lr: 4.1578e-05\n",
      "Epoch 91/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4867\n",
      "Epoch 91: ReduceLROnPlateau reducing learning rate to 3.752413140318822e-05.\n",
      "13/13 [==============================] - 2s 171ms/step - loss: 0.4867 - val_loss: 0.4581 - lr: 3.9499e-05\n",
      "Epoch 92/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4880\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 3.564792586985277e-05.\n",
      "13/13 [==============================] - 2s 136ms/step - loss: 0.4880 - val_loss: 0.4558 - lr: 3.7524e-05\n",
      "Epoch 93/100\n",
      "13/13 [==============================] - ETA: 0s - loss: 0.4851\n",
      "Epoch 93: ReduceLROnPlateau reducing learning rate to 3.3865528712340166e-05.\n",
      "Restoring model weights from the end of the best epoch: 68.\n",
      "13/13 [==============================] - 3s 204ms/step - loss: 0.4851 - val_loss: 0.4531 - lr: 3.5648e-05\n",
      "Epoch 93: early stopping\n",
      "97/97 [==============================] - 15s 15ms/step\n",
      "min_train_loss: 0.4878, min_val_loss: 0.4467, bll: 0.4467\n",
      "______fold 1______, ________repeat 2__________\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:45\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/engine/training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1740\u001b[0m ):\n\u001b[1;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:873\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m   \u001b[39m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    872\u001b[0m   initializers \u001b[39m=\u001b[39m []\n\u001b[0;32m--> 873\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwds, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[1;32m    874\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    875\u001b[0m   \u001b[39m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    876\u001b[0m   \u001b[39m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    877\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:694\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[39m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m    693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 694\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    695\u001b[0m     \u001b[39m.\u001b[39;49m_get_concrete_function_internal_garbage_collected(\n\u001b[1;32m    696\u001b[0m         \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds))\n\u001b[1;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[1;32m    699\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:176\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m--> 176\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[1;32m    169\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:398\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    395\u001b[0m args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[1;32m    396\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n\u001b[0;32m--> 398\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_concrete_function(\n\u001b[1;32m    399\u001b[0m     args, kwargs, func_graph)\n\u001b[1;32m    401\u001b[0m \u001b[39m# TODO(b/263520817): Remove access to private attribute.\u001b[39;00m\n\u001b[1;32m    402\u001b[0m graph_capture_container \u001b[39m=\u001b[39m concrete_function\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:305\u001b[0m, in \u001b[0;36mTracingCompiler._create_concrete_function\u001b[0;34m(self, args, kwargs, func_graph)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    302\u001b[0m   arg_names \u001b[39m=\u001b[39m base_arg_names\n\u001b[1;32m    304\u001b[0m concrete_function \u001b[39m=\u001b[39m monomorphic_function\u001b[39m.\u001b[39mConcreteFunction(\n\u001b[0;32m--> 305\u001b[0m     func_graph_module\u001b[39m.\u001b[39;49mfunc_graph_from_py_func(\n\u001b[1;32m    306\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_name,\n\u001b[1;32m    307\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_python_function,\n\u001b[1;32m    308\u001b[0m         args,\n\u001b[1;32m    309\u001b[0m         kwargs,\n\u001b[1;32m    310\u001b[0m         \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    311\u001b[0m         func_graph\u001b[39m=\u001b[39;49mfunc_graph,\n\u001b[1;32m    312\u001b[0m         arg_names\u001b[39m=\u001b[39;49marg_names,\n\u001b[1;32m    313\u001b[0m         capture_by_value\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_capture_by_value,\n\u001b[1;32m    314\u001b[0m         create_placeholders\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m    315\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_attributes,\n\u001b[1;32m    316\u001b[0m     spec\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_spec,\n\u001b[1;32m    317\u001b[0m     \u001b[39m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[39m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[39m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[1;32m    320\u001b[0m     \u001b[39m# ConcreteFunction.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     shared_func_graph\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    322\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:1055\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1052\u001b[0m   \u001b[39mreturn\u001b[39;00m x\n\u001b[1;32m   1054\u001b[0m _, original_func \u001b[39m=\u001b[39m tf_decorator\u001b[39m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1055\u001b[0m func_outputs \u001b[39m=\u001b[39m python_func(\u001b[39m*\u001b[39;49mfunc_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfunc_kwargs)\n\u001b[1;32m   1057\u001b[0m \u001b[39m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[39m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m func_outputs \u001b[39m=\u001b[39m variable_utils\u001b[39m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:597\u001b[0m, in \u001b[0;36mFunction._compiler_with_scope.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[39mwith\u001b[39;00m default_graph\u001b[39m.\u001b[39m_variable_creator_scope(scope, priority\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m):  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    594\u001b[0m   \u001b[39m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[39m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[39mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 597\u001b[0m     out \u001b[39m=\u001b[39m weak_wrapped_fn()\u001b[39m.\u001b[39;49m__wrapped__(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    598\u001b[0m   \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m   \u001b[39mreturn\u001b[39;00m api\u001b[39m.\u001b[39;49mconverted_call(\n\u001b[1;32m     42\u001b[0m       original_func,\n\u001b[1;32m     43\u001b[0m       args,\n\u001b[1;32m     44\u001b[0m       kwargs,\n\u001b[1;32m     45\u001b[0m       options\u001b[39m=\u001b[39;49mconverter\u001b[39m.\u001b[39;49mConversionOptions(\n\u001b[1;32m     46\u001b[0m           recursive\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     47\u001b[0m           optional_features\u001b[39m=\u001b[39;49mautograph_options,\n\u001b[1;32m     48\u001b[0m           user_requested\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     49\u001b[0m       ))\n\u001b[1;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m\"\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39;49meffective_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    440\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[39m=\u001b[39m converted_f(\u001b[39m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/var/folders/s1/w69x8mm13wz_1npk05p7rzy00000gn/T/__autograph_generated_fileulo2bsdg.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39;49mconverted_call(ag__\u001b[39m.\u001b[39;49mld(step_function), (ag__\u001b[39m.\u001b[39;49mld(\u001b[39mself\u001b[39;49m), ag__\u001b[39m.\u001b[39;49mld(iterator)), \u001b[39mNone\u001b[39;49;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:460\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/engine/training.py:1322\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     run_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mfunction(\n\u001b[1;32m   1319\u001b[0m         run_step, jit_compile\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, reduce_retracing\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1320\u001b[0m     )\n\u001b[1;32m   1321\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1322\u001b[0m outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mdistribute_strategy\u001b[39m.\u001b[39;49mrun(run_step, args\u001b[39m=\u001b[39;49m(data,))\n\u001b[1;32m   1323\u001b[0m outputs \u001b[39m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1324\u001b[0m     outputs,\n\u001b[1;32m   1325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1326\u001b[0m     reduction\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1327\u001b[0m )\n\u001b[1;32m   1328\u001b[0m \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1673\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1668\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscope():\n\u001b[1;32m   1669\u001b[0m   \u001b[39m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m   \u001b[39m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   1672\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 1673\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_extended\u001b[39m.\u001b[39;49mcall_for_each_replica(fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3250\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3248\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   3249\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> 3250\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:4048\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4046\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call_for_each_replica\u001b[39m(\u001b[39mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   4047\u001b[0m   \u001b[39mwith\u001b[39;00m ReplicaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m-> 4048\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/engine/training.py:1303\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1303\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain_step(data)\n\u001b[1;32m   1304\u001b[0m     \u001b[39m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/engine/training.py:1084\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_target_and_loss(y, loss)\n\u001b[1;32m   1083\u001b[0m \u001b[39m# Run backwards pass.\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mminimize(loss, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainable_variables, tape\u001b[39m=\u001b[39;49mtape)\n\u001b[1;32m   1085\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_metrics(x, y, y_pred, sample_weight)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py:544\u001b[0m, in \u001b[0;36m_BaseOptimizer.minimize\u001b[0;34m(self, loss, var_list, tape)\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Minimize `loss` by updating `var_list`.\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \n\u001b[1;32m    525\u001b[0m \u001b[39mThis method simply computes gradient using `tf.GradientTape` and calls\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[39m  None\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    543\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_gradients(loss, var_list, tape)\n\u001b[0;32m--> 544\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py:1230\u001b[0m, in \u001b[0;36mOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name, skip_gradients_aggregation, **kwargs)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_gradients_aggregation \u001b[39mand\u001b[39;00m experimental_aggregate_gradients:\n\u001b[1;32m   1229\u001b[0m     grads_and_vars \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregate_gradients(grads_and_vars)\n\u001b[0;32m-> 1230\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply_gradients(grads_and_vars, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py:652\u001b[0m, in \u001b[0;36m_BaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars, name)\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_weight_decay(trainable_variables)\n\u001b[1;32m    651\u001b[0m grads_and_vars \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(grads, trainable_variables))\n\u001b[0;32m--> 652\u001b[0m iteration \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_internal_apply_gradients(grads_and_vars)\n\u001b[1;32m    654\u001b[0m \u001b[39m# Apply variable constraints after applying gradients.\u001b[39;00m\n\u001b[1;32m    655\u001b[0m \u001b[39mfor\u001b[39;00m variable \u001b[39min\u001b[39;00m trainable_variables:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py:1260\u001b[0m, in \u001b[0;36mOptimizer._internal_apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mesh \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_with_dtensor:\n\u001b[1;32m   1257\u001b[0m     \u001b[39m# Skip any usage of strategy logic for DTensor\u001b[39;00m\n\u001b[1;32m   1258\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m_internal_apply_gradients(grads_and_vars)\n\u001b[0;32m-> 1260\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49m__internal__\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49minterim\u001b[39m.\u001b[39;49mmaybe_merge_call(\n\u001b[1;32m   1261\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distributed_apply_gradients_fn,\n\u001b[1;32m   1262\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_distribution_strategy,\n\u001b[1;32m   1263\u001b[0m     grads_and_vars,\n\u001b[1;32m   1264\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/merge_call_interim.py:51\u001b[0m, in \u001b[0;36mmaybe_merge_call\u001b[0;34m(fn, strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Maybe invoke `fn` via `merge_call` which may or may not be fulfilled.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \n\u001b[1;32m     33\u001b[0m \u001b[39mThe caller of this utility function requests to invoke `fn` via `merge_call`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m  The return value of the `fn` call.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m strategy_supports_no_merge_call():\n\u001b[0;32m---> 51\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(strategy, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   \u001b[39mreturn\u001b[39;00m distribute_lib\u001b[39m.\u001b[39mget_replica_context()\u001b[39m.\u001b[39mmerge_call(\n\u001b[1;32m     54\u001b[0m       fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py:1352\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn\u001b[0;34m(self, distribution, grads_and_vars, **kwargs)\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step(grad, var)\n\u001b[1;32m   1351\u001b[0m \u001b[39mfor\u001b[39;00m grad, var \u001b[39min\u001b[39;00m grads_and_vars:\n\u001b[0;32m-> 1352\u001b[0m     distribution\u001b[39m.\u001b[39;49mextended\u001b[39m.\u001b[39;49mupdate(\n\u001b[1;32m   1353\u001b[0m         var, apply_grad_to_update_var, args\u001b[39m=\u001b[39;49m(grad,), group\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m   1354\u001b[0m     )\n\u001b[1;32m   1356\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_ema:\n\u001b[1;32m   1357\u001b[0m     _, var_list \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mgrads_and_vars)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2994\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2992\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update(var, fn, args, kwargs, group)\n\u001b[1;32m   2993\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2994\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_replica_ctx_update(\n\u001b[1;32m   2995\u001b[0m       var, fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs, group\u001b[39m=\u001b[39;49mgroup)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2873\u001b[0m, in \u001b[0;36mStrategyExtendedV2._replica_ctx_update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2870\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_fn\u001b[39m(_, \u001b[39m*\u001b[39mmerged_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerged_kwargs):\n\u001b[1;32m   2871\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate(var, fn, merged_args, merged_kwargs, group\u001b[39m=\u001b[39mgroup)\n\u001b[0;32m-> 2873\u001b[0m \u001b[39mreturn\u001b[39;00m replica_context\u001b[39m.\u001b[39;49mmerge_call(merge_fn, args\u001b[39m=\u001b[39;49margs, kwargs\u001b[39m=\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3465\u001b[0m, in \u001b[0;36mReplicaContextBase.merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m   3463\u001b[0m merge_fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   3464\u001b[0m     merge_fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m-> 3465\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_merge_call(merge_fn, args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3472\u001b[0m, in \u001b[0;36mReplicaContextBase._merge_call\u001b[0;34m(self, merge_fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3469\u001b[0m _push_per_thread_mode(  \u001b[39m# thread-local, so not needed with multiple threads\u001b[39;00m\n\u001b[1;32m   3470\u001b[0m     _CrossReplicaThreadMode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy))  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   3471\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3472\u001b[0m   \u001b[39mreturn\u001b[39;00m merge_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3473\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   3474\u001b[0m   _pop_per_thread_mode()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39muser_requested \u001b[39mand\u001b[39;00m conversion\u001b[39m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    379\u001b[0m \u001b[39m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[39m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[39m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m options\u001b[39m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2871\u001b[0m, in \u001b[0;36mStrategyExtendedV2._replica_ctx_update.<locals>.merge_fn\u001b[0;34m(_, *merged_args, **merged_kwargs)\u001b[0m\n\u001b[1;32m   2870\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge_fn\u001b[39m(_, \u001b[39m*\u001b[39mmerged_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmerged_kwargs):\n\u001b[0;32m-> 2871\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate(var, fn, merged_args, merged_kwargs, group\u001b[39m=\u001b[39;49mgroup)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2992\u001b[0m, in \u001b[0;36mStrategyExtendedV2.update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   2989\u001b[0m   fn \u001b[39m=\u001b[39m autograph\u001b[39m.\u001b[39mtf_convert(\n\u001b[1;32m   2990\u001b[0m       fn, autograph_ctx\u001b[39m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m   2991\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_container_strategy()\u001b[39m.\u001b[39mscope():\n\u001b[0;32m-> 2992\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update(var, fn, args, kwargs, group)\n\u001b[1;32m   2993\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   2994\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_replica_ctx_update(\n\u001b[1;32m   2995\u001b[0m       var, fn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs, group\u001b[39m=\u001b[39mgroup)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:4062\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update\u001b[0;34m(self, var, fn, args, kwargs, group)\u001b[0m\n\u001b[1;32m   4059\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update\u001b[39m(\u001b[39mself\u001b[39m, var, fn, args, kwargs, group):\n\u001b[1;32m   4060\u001b[0m   \u001b[39m# The implementations of _update() and _update_non_slot() are identical\u001b[39;00m\n\u001b[1;32m   4061\u001b[0m   \u001b[39m# except _update() passes `var` as the first argument to `fn()`.\u001b[39;00m\n\u001b[0;32m-> 4062\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_non_slot(var, fn, (var,) \u001b[39m+\u001b[39;49m \u001b[39mtuple\u001b[39;49m(args), kwargs, group)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:4068\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._update_non_slot\u001b[0;34m(self, colocate_with, fn, args, kwargs, should_group)\u001b[0m\n\u001b[1;32m   4064\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_non_slot\u001b[39m(\u001b[39mself\u001b[39m, colocate_with, fn, args, kwargs, should_group):\n\u001b[1;32m   4065\u001b[0m   \u001b[39m# TODO(josh11b): Figure out what we should be passing to UpdateContext()\u001b[39;00m\n\u001b[1;32m   4066\u001b[0m   \u001b[39m# once that value is used for something.\u001b[39;00m\n\u001b[1;32m   4067\u001b[0m   \u001b[39mwith\u001b[39;00m UpdateContext(colocate_with):\n\u001b[0;32m-> 4068\u001b[0m     result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   4069\u001b[0m     \u001b[39mif\u001b[39;00m should_group:\n\u001b[1;32m   4070\u001b[0m       \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[39mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mreturn\u001b[39;00m converted_call(f, args, kwargs, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m    691\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(e, \u001b[39m'\u001b[39m\u001b[39mag_error_metadata\u001b[39m\u001b[39m'\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:331\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[39mif\u001b[39;00m conversion\u001b[39m.\u001b[39mis_in_allowlist_cache(f, options):\n\u001b[1;32m    330\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: from cache\u001b[39m\u001b[39m'\u001b[39m, f)\n\u001b[0;32m--> 331\u001b[0m   \u001b[39mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options, \u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    333\u001b[0m \u001b[39mif\u001b[39;00m ag_ctx\u001b[39m.\u001b[39mcontrol_status_ctx()\u001b[39m.\u001b[39mstatus \u001b[39m==\u001b[39m ag_ctx\u001b[39m.\u001b[39mStatus\u001b[39m.\u001b[39mDISABLED:\n\u001b[1;32m    334\u001b[0m   logging\u001b[39m.\u001b[39mlog(\u001b[39m2\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAllowlisted: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: AutoGraph is disabled in context\u001b[39m\u001b[39m'\u001b[39m, f)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[39mreturn\u001b[39;00m f\u001b[39m.\u001b[39m\u001b[39m__self__\u001b[39m\u001b[39m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    460\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39margs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py:1349\u001b[0m, in \u001b[0;36mOptimizer._distributed_apply_gradients_fn.<locals>.apply_grad_to_update_var\u001b[0;34m(var, grad)\u001b[0m\n\u001b[1;32m   1347\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_step_xla(grad, var, \u001b[39mid\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(var)))\n\u001b[1;32m   1348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_step(grad, var)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/optimizers/optimizer.py:241\u001b[0m, in \u001b[0;36m_BaseOptimizer._update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_key(variable) \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_dict:\n\u001b[1;32m    233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\n\u001b[1;32m    234\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe optimizer cannot recognize variable \u001b[39m\u001b[39m{\u001b[39;00mvariable\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    235\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis usually means you are trying to call the optimizer to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`tf.keras.optimizers.legacy.\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    240\u001b[0m     )\n\u001b[0;32m--> 241\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mupdate_step(gradient, variable)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/keras/src/optimizers/adam.py:166\u001b[0m, in \u001b[0;36mAdam.update_step\u001b[0;34m(self, gradient, variable)\u001b[0m\n\u001b[1;32m    164\u001b[0m beta_2_power \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    165\u001b[0m lr \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate, variable\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 166\u001b[0m local_step \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcast(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterations \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, variable\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    167\u001b[0m beta_1_power \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mpow(tf\u001b[39m.\u001b[39mcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_1, variable\u001b[39m.\u001b[39mdtype), local_step)\n\u001b[1;32m    168\u001b[0m beta_2_power \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mpow(tf\u001b[39m.\u001b[39mcast(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbeta_2, variable\u001b[39m.\u001b[39mdtype), local_step)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/ops/variables.py:1011\u001b[0m, in \u001b[0;36mVariable._OverloadOperator.<locals>._run_op\u001b[0;34m(a, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_op\u001b[39m(a, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1010\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m   \u001b[39mreturn\u001b[39;00m tensor_oper(a\u001b[39m.\u001b[39;49mvalue(), \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1466\u001b[0m, in \u001b[0;36m_OverrideBinaryOperatorHelper.<locals>.binary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1461\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1462\u001b[0m   \u001b[39m# force_same_dtype=False to preserve existing TF behavior\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m   \u001b[39m# TODO(b/178860388): Figure out why binary_op_wrapper and\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m   \u001b[39m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[39;00m\n\u001b[1;32m   1465\u001b[0m   x, y \u001b[39m=\u001b[39m maybe_promote_tensors(x, y)\n\u001b[0;32m-> 1466\u001b[0m   \u001b[39mreturn\u001b[39;00m func(x, y, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m   1467\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   1468\u001b[0m   \u001b[39m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[39;00m\n\u001b[1;32m   1469\u001b[0m   \u001b[39m# object that can implement the operator with knowledge of itself\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1472\u001b[0m   \u001b[39m# original error from the LHS, because it may be more\u001b[39;00m\n\u001b[1;32m   1473\u001b[0m   \u001b[39m# informative.\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mtype\u001b[39m(y), \u001b[39m\"\u001b[39m\u001b[39m__r\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m op_name):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[39m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[39mreturn\u001b[39;00m dispatch_target(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1177\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mTypeError\u001b[39;00m, \u001b[39mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[39m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[39m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[39m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/ops/math_ops.py:1837\u001b[0m, in \u001b[0;36m_add_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1835\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39madd(x, y, name\u001b[39m=\u001b[39mname)\n\u001b[1;32m   1836\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1837\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_math_ops\u001b[39m.\u001b[39;49madd_v2(x, y, name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/ops/gen_math_ops.py:475\u001b[0m, in \u001b[0;36madd_v2\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[39m# Add nodes to the TensorFlow graph.\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m _, _, _op, _outputs \u001b[39m=\u001b[39m _op_def_library\u001b[39m.\u001b[39;49m_apply_op_helper(\n\u001b[1;32m    476\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mAddV2\u001b[39;49m\u001b[39m\"\u001b[39;49m, x\u001b[39m=\u001b[39;49mx, y\u001b[39m=\u001b[39;49my, name\u001b[39m=\u001b[39;49mname)\n\u001b[1;32m    477\u001b[0m _result \u001b[39m=\u001b[39m _outputs[:]\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m _execute\u001b[39m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/framework/op_def_library.py:795\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    790\u001b[0m must_colocate_inputs \u001b[39m=\u001b[39m [val \u001b[39mfor\u001b[39;00m arg, val \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(op_def\u001b[39m.\u001b[39minput_arg, inputs)\n\u001b[1;32m    791\u001b[0m                         \u001b[39mif\u001b[39;00m arg\u001b[39m.\u001b[39mis_ref]\n\u001b[1;32m    792\u001b[0m \u001b[39mwith\u001b[39;00m _MaybeColocateWith(must_colocate_inputs):\n\u001b[1;32m    793\u001b[0m   \u001b[39m# Add Op to graph\u001b[39;00m\n\u001b[1;32m    794\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 795\u001b[0m   op \u001b[39m=\u001b[39m g\u001b[39m.\u001b[39;49m_create_op_internal(op_type_name, inputs, dtypes\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    796\u001b[0m                              name\u001b[39m=\u001b[39;49mscope, input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[1;32m    797\u001b[0m                              attrs\u001b[39m=\u001b[39;49mattr_protos, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[1;32m    799\u001b[0m \u001b[39m# `outputs` is returned as a separate return value so that the output\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[39m# tensors can the `op` per se can be decoupled so that the\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39m# `op_callbacks` can function properly. See framework/op_callbacks.py\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m# for more details.\u001b[39;00m\n\u001b[1;32m    803\u001b[0m outputs \u001b[39m=\u001b[39m op\u001b[39m.\u001b[39moutputs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m    668\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[1;32m    669\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[0;32m--> 670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[1;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:3381\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[0;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[1;32m   3378\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[1;32m   3379\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[1;32m   3380\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[0;32m-> 3381\u001b[0m   ret \u001b[39m=\u001b[39m Operation\u001b[39m.\u001b[39;49mfrom_node_def(\n\u001b[1;32m   3382\u001b[0m       node_def,\n\u001b[1;32m   3383\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   3384\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m   3385\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[1;32m   3386\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[1;32m   3387\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[1;32m   3388\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[1;32m   3389\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def,\n\u001b[1;32m   3390\u001b[0m   )\n\u001b[1;32m   3391\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[1;32m   3392\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1889\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1886\u001b[0m     control_input_ops\u001b[39m.\u001b[39mappend(control_op)\n\u001b[1;32m   1888\u001b[0m \u001b[39m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[0;32m-> 1889\u001b[0m c_op \u001b[39m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[1;32m   1890\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Operation(c_op, GraphTensor)\n\u001b[1;32m   1891\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init(g)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:1732\u001b[0m, in \u001b[0;36m_create_c_op\u001b[0;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[1;32m   1729\u001b[0m     pywrap_tf_session\u001b[39m.\u001b[39mTF_AddInputList(op_desc,\n\u001b[1;32m   1730\u001b[0m                                       [t\u001b[39m.\u001b[39m_as_tf_output() \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m op_input])\n\u001b[1;32m   1731\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1732\u001b[0m     pywrap_tf_session\u001b[39m.\u001b[39mTF_AddInput(op_desc, op_input\u001b[39m.\u001b[39;49m_as_tf_output())\n\u001b[1;32m   1734\u001b[0m \u001b[39m# Add control inputs\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[39mfor\u001b[39;00m control_input \u001b[39min\u001b[39;00m control_inputs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import optimizers as O\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "blls = []\n",
    "\n",
    "batch_size = 1000#32\n",
    "\n",
    "units_1 = 32\n",
    "drop_1 = 0.75\n",
    "dense_units = 8\n",
    "\n",
    "units_2 = 16\n",
    "drop_2 = 0.5\n",
    "\n",
    "units_3 = 8\n",
    "drop_3 = 0.25\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)#StratifiedKFold(n_splits=5, shuffle=True, random_state=722)\n",
    "X = trainX.reset_index(drop=True).values\n",
    "y = trainy.reset_index(drop=True).values\n",
    "for n, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    for k in range(3):\n",
    "        print(f'______fold {n+1}______, ________repeat {k+1}__________')\n",
    "        inputs_1 = tf.keras.Input(shape=(trainX.shape[1],))\n",
    "        \n",
    "        features_1 = VariableSelectionFlow(trainX.shape[1], units_1, drop_1, dense_units=dense_units)(inputs_1)\n",
    "        features_2 = VariableSelectionFlow(units_1, units_2, drop_2)(features_1)         \n",
    "        features_3 = VariableSelectionFlow(units_2, units_3, drop_3)(features_2)         \n",
    "\n",
    "        outputs = L.Dense(1)(features_3)\n",
    "\n",
    "        model = Model(inputs=inputs_1, outputs=outputs)      \n",
    "\n",
    "        opt = O.Adam(1e-3, epsilon=1e-7)\n",
    "        loss = MeanSquaredError()\n",
    "\n",
    "        lr = ReduceLROnPlateau(monitor=\"val_loss\", mode='min', factor=0.95, patience=1, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', patience=25, verbose=1, restore_best_weights=True)\n",
    "\n",
    "        model.compile(optimizer=opt, loss=loss)\n",
    "        history = model.fit(x=X[train_idx], y=y[train_idx],\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=100,\n",
    "                          validation_data=(X[val_idx], y[val_idx]),\n",
    "                          callbacks=[lr, es])\n",
    "                \n",
    "        bll = loss(y[val_idx], model.predict(X[val_idx]))\n",
    "        blls.append(bll)\n",
    "        val_loss = np.asarray(history.history['val_loss'])\n",
    "        train_loss = np.asarray(history.history['loss'])\n",
    "        min_val_loss = val_loss.min()\n",
    "        min_train_loss = train_loss[val_loss.argmin()]\n",
    "        print(f'min_train_loss: {min_train_loss:.4f}, min_val_loss: {min_val_loss:.4f}, bll: {bll:.4f}')  \n",
    "        \n",
    "        #model.save_weights(f'./models_weights/mod_f{n}_r{k}.h5')\n",
    "        \n",
    "print(np.mean(blls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(val_loss)\n",
    "plt.plot(train_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "mfolder = './'\n",
    "models_weights = os.listdir(mfolder)\n",
    "r = re.compile(\".*h5\")\n",
    "models_weights = list(filter(r.match, models_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 11s 13ms/step\n",
      "162/162 [==============================] - 9s 13ms/step\n",
      "162/162 [==============================] - 13s 24ms/step\n",
      "162/162 [==============================] - 15s 25ms/step\n",
      "162/162 [==============================] - 13s 15ms/step\n",
      "162/162 [==============================] - 14s 18ms/step\n",
      "162/162 [==============================] - 10s 13ms/step\n",
      "162/162 [==============================] - 12s 22ms/step\n",
      "162/162 [==============================] - 10s 11ms/step\n",
      "162/162 [==============================] - 11s 13ms/step\n",
      "162/162 [==============================] - 12s 12ms/step\n",
      "162/162 [==============================] - 10s 14ms/step\n",
      "162/162 [==============================] - 11s 15ms/step\n",
      "162/162 [==============================] - 10s 13ms/step\n",
      "162/162 [==============================] - 12s 22ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros_like(testy.iloc[:,0].values, dtype=np.float32)\n",
    "batch_size = 32\n",
    "\n",
    "units_1 = 32\n",
    "drop_1 = 0.75\n",
    "dense_units = 8\n",
    "\n",
    "units_2 = 16\n",
    "drop_2 = 0.5\n",
    "\n",
    "units_3 = 8\n",
    "drop_3 = 0.25\n",
    "predys = [[0]*len(testX) for _ in range(len(models_weights))]\n",
    "\n",
    "for n, model_weights in enumerate(models_weights):\n",
    "    inputs_1 = tf.keras.Input(shape=(testX.shape[1],))\n",
    "    \n",
    "    features_1 = VariableSelectionFlow(testX.shape[1], units_1, drop_1, dense_units=dense_units)(inputs_1)\n",
    "    features_2 = VariableSelectionFlow(units_1, units_2, drop_2)(features_1)\n",
    "    features_3 = VariableSelectionFlow(units_2, units_3, drop_3)(features_2)\n",
    "\n",
    "    outputs = L.Dense(1)(features_3)\n",
    "\n",
    "    model = Model(inputs=inputs_1, outputs=outputs)\n",
    "    model.load_weights(mfolder + model_weights)\n",
    "    predys[n] = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predy = np.median(predys, axis=0).ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4144616452839398"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(testy, predy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
