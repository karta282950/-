{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukaisun/opt/anaconda3/lib/python3.8/site-packages/xgboost/compat.py:31: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "/Users/yukaisun/opt/anaconda3/lib/python3.8/site-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/Users/yukaisun/opt/anaconda3/lib/python3.8/site-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.Float64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n",
      "/Users/yukaisun/opt/anaconda3/lib/python3.8/site-packages/dask/dataframe/utils.py:367: FutureWarning: pandas.UInt64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  _numeric_index_types = (pd.Int64Index, pd.Float64Index, pd.UInt64Index)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning models that we'll use\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm\n",
    "#from sklearn.linear_model import Ridge\n",
    "import catboost as cat\n",
    "# optuna\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>MedHouseVal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  MedHouseVal  \n",
       "0    -122.23        4.526  \n",
       "1    -122.22        3.585  \n",
       "2    -122.24        3.521  \n",
       "3    -122.25        3.413  \n",
       "4    -122.25        3.422  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We set as_frame parameter to True and access the return object's \"frame\"\n",
    "# attribute to get the dataset as pandas dataframe.\n",
    "\n",
    "df = fetch_california_housing(as_frame=True)[\"frame\"]\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(df.drop(columns=['MedHouseVal']), df.MedHouseVal, test_size=0.15, shuffle=True, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "n_folds = 10\n",
    "def CV(model, X, y, loss_function):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    kfold = kf.split(X, y)\n",
    "    losses = []\n",
    "    for (train_idx, test_idx) in kfold:\n",
    "        trainX, trainy, testX, testy = X.iloc[train_idx], y.iloc[test_idx], X.iloc[test_idx], y.iloc[test_idx]\n",
    "        model.fit(trainX, trainy)\n",
    "        predy = model.predict_proba(testX)\n",
    "        loss = loss_function(testy, predy)\n",
    "        losses.append(loss)\n",
    "    return np.sum(losses) / n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Best parameter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import optuna\n",
    "\n",
    "def objective(trial, data=X, target=y, param, model):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15, random_state=42)\n",
    "    amodel = model(**param)\n",
    "    amodel.fit(train_x, train_y, eval_set=[(test_x,test_y)], early_stopping_rounds=100, verbose=False)\n",
    "    preds = amodel.predict(test_x)\n",
    "    rmse = mean_squared_error(test_y, preds,squared=False)\n",
    "    return rmse\n",
    "\n",
    "def objective1(trial):\n",
    "    param = {'n_estimators': trial.suggest_int('n_estimators', 50, 200, 10),\n",
    "            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n",
    "            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n",
    "            'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]),\n",
    "            'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1),\n",
    "            'max_depth': trial.suggest_int('max_depth', 3, 20),\n",
    "            'num_leaves' : trial.suggest_int('num_leaves', 10, 50),\n",
    "            'min_child_samples': trial.suggest_int('min_child_samples', 1, 50)}\n",
    "        \n",
    "    model = LGBMClassifier(**param, random_state = 42)\n",
    "    score = CV(model, trainX, mean_squared_error)\n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='minimize', study_name=\"LightGBM\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "#optuna.visualization.plot_optimization_history(study)\n",
    "#optuna.visualization.plot_parallel_coordinate(study)\n",
    "#optuna.visualization.plot_param_importances(study)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukaisun/opt/anaconda3/lib/python3.8/site-packages/xgboost/data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Average Ensemble's MSE: 0.20581131648521198\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# instantiating the model\n",
    "cat_model = cat.CatBoostRegressor(verbose=False)\n",
    "cat_model.fit(trainX, trainy)\n",
    "y_pred_cat = cat_model.predict(testX)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor()\n",
    "xgb_model.fit(trainX, trainy, verbose=False)\n",
    "y_pred_xgb = xgb_model.predict(testX)\n",
    "\n",
    "lgbm_model = lgbm.LGBMRegressor()\n",
    "lgbm_model.fit(trainX, trainy, verbose=-1)\n",
    "y_pred_lgbm = lgbm_model.predict(testX)\n",
    "\n",
    "# combining predictions by taking simple average using numpy\n",
    "y_pred_final = np.mean([y_pred_cat, y_pred_xgb, y_pred_lgbm], axis=0)\n",
    "\n",
    "# let's calculate mse\n",
    "mse = mean_squared_error(testy, y_pred_lgbm)\n",
    "\n",
    "print(f\"Simple Average Ensemble's MSE: {mse}\") #Simple Average Ensemble's MSE: 0.20581131648521198"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best parameter\n",
    "```Python\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "def objective(trial,data=X,target=y):\n",
    "    \n",
    "    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n",
    "    param = {\n",
    "        #'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n",
    "        #'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n",
    "        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n",
    "        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n",
    "        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.01,0.012,0.014,0.016,0.018, 0.02]),\n",
    "        'n_estimators': 10000,\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17]),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**param)  \n",
    "    callbacks = [lgb.early_stopping(10, verbose=0)]#, lgb.log_evaluation(period=0)]\n",
    "    model.fit(train_x,train_y,eval_set=[(test_x,test_y)], early_stopping_rounds=100,verbose=False)\n",
    "    \n",
    "    preds = model.predict(test_x)\n",
    "    \n",
    "    rmse = mean_squared_error(test_y, preds,squared=False)\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "  STEP_SIZE = 1\n",
    "\n",
    "  weights = []\n",
    "  all_models_predictions = []\n",
    "\n",
    "  # we'll use a variable for setting upper limit for suggested value\n",
    "  # since we intend to update it after each weight suggestion\n",
    "  upper_limit = 100\n",
    "\n",
    "  w_cat = trial.suggest_int(\"w_cat\", 0, upper_limit, step=STEP_SIZE)\n",
    "  weights.append(w_cat)\n",
    "\n",
    "  # Update upper limit to 100 - all the previous weights combined, which in this case is just w_ridge\n",
    "  # WHY? well because we want to keep our sum of all weights equal to 100\n",
    "  # and this is one way of ensuring that!\n",
    "  upper_limit -= sum(weights)\n",
    "  upper_limit = upper_limit\n",
    "\n",
    "  w_xgb = trial.suggest_int(\"w_xgb\", 0, upper_limit, step=STEP_SIZE)\n",
    "  weights.append(w_xgb)\n",
    "\n",
    "  # for the final weight we won't use optuna, rather we'll manually set it equal\n",
    "  # to whatever value remains after subtracting the sum of suggested weight values from 100\n",
    "  # This will also make sure that the sum of all weights remains equal to 100.\n",
    "  w_lgbm = 100 - sum(weights)\n",
    "  weights.append(w_lgbm)\n",
    "\n",
    "  # Just as a sanity check, we'll check that the sum of all weights is equal to 100\n",
    "  weights_sum = sum(weights)\n",
    "\n",
    "  if weights_sum != 100:\n",
    "    raise Exception(f\"Weights sum must be equal to 100. Instead {weights_sum} was encountered!\")\n",
    "  \n",
    "  # We'll use the default parameter values for all our models\n",
    "  cat_model = cat.CatBoostRegressor()\n",
    "  cat_model.fit(X_train, y_train)\n",
    "  y_pred_ridge = cat_model.predict(X_val)\n",
    "  all_models_predictions.append(y_pred_ridge)\n",
    "\n",
    "  xgb_model = xgb.XGBRegressor()\n",
    "  xgb_model.fit(X_train, y_train)\n",
    "  y_pred_xgb = xgb_model.predict(X_val)\n",
    "  all_models_predictions.append(y_pred_xgb)\n",
    "\n",
    "  lgbm_model = lgbm.LGBMRegressor()\n",
    "  lgbm_model.fit(X_train, y_train, verbose=-1)\n",
    "  y_pred_lgbm = lgbm_model.predict(X_val)\n",
    "  all_models_predictions.append(y_pred_lgbm)\n",
    "\n",
    "  # let's take the weighted average of the predictions using numpy\n",
    "  y_pred_final = np.average(all_models_predictions, weights=weights, axis=0)\n",
    "  # computing our metric i.e. MSE\n",
    "  mse = mean_squared_error(y_val, y_pred_final)\n",
    "\n",
    "  return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(study_name=\"optimizing weights\", direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "print('Best trial:', study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna best output\n",
    "```Python\n",
    "[I 2023-07-20 13:00:07,167] Trial 14 finished with value: 0.1882958572840078 and parameters: {'w_cat': 78, 'w_xgb': 17}. Best is trial 13 with value: 0.18818298170682904.\n",
    "/Users/yukaisun/opt/anaconda3/lib/python3.8/site-packages/catboost/core.py:1133: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
    "  self._init_pool(data, label, cat_features, text_features, embedding_features, pairs, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, feature_names, thread_count)\n",
    "Learning rate set to 0.064294\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#{'w_ridge': 10, 'w_xgb': 80}. Best is trial 14 with value: 0.2082563139755657.\n",
    "cat_model = cat.CatBoostRegressor()\n",
    "# training the model\n",
    "cat_model.fit(X_train, y_train)\n",
    "# predicting using the trained model\n",
    "y_pred_cat = cat_model.predict(X_val)\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\")\n",
    "xgb_model.fit(X_train, y_train, verbose=False)\n",
    "y_pred_xgb = xgb_model.predict(X_val)\n",
    "\n",
    "lgbm_model = lgbm.LGBMRegressor()\n",
    "lgbm_model.fit(X_train, y_train, verbose=-1)\n",
    "y_pred_lgbm = lgbm_model.predict(X_val)\n",
    "\n",
    "# combining predictions by taking simple average using numpy\n",
    "y_pred_final = 0.78*y_pred_cat + 0.17*y_pred_xgb + 0.05*y_pred_lgbm\n",
    "\n",
    "# let's calculate mse\n",
    "mse = mean_squared_error(y_val, y_pred_final)\n",
    "\n",
    "print(f\"Simple Average Ensemble's MSE: {mse}\")\n",
    "# Simple Average Ensemble's MSE: 0.1882958570766543"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
