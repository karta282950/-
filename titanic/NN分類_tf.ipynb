{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "data = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "data['target'] = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Creating the Layers for the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-21 16:24:59.930962: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers as L\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "def smish(x):\n",
    "    return x * K.tanh(K.log(1 + K.sigmoid(x)))\n",
    "\n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class GatedLinearUnit(L.Layer):\n",
    "    def __init__(self, units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.linear = L.Dense(units)\n",
    "        self.sigmoid = L.Dense(units, activation=\"sigmoid\")\n",
    "        self.units = units\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.linear(inputs) * self.sigmoid(inputs)\n",
    "    \n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class GatedResidualNetwork(L.Layer):\n",
    "    def __init__(self, units, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.relu_dense = L.Dense(units, activation=smish)\n",
    "        self.linear_dense = L.Dense(units)\n",
    "        self.dropout = L.Dropout(dropout_rate)\n",
    "        self.gated_linear_unit = GatedLinearUnit(units)\n",
    "        self.layer_norm = L.LayerNormalization()\n",
    "        self.project = L.Dense(units)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['units'] = self.units\n",
    "        config['dropout_rate'] = self.dropout_rate\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.relu_dense(inputs)\n",
    "        x = self.linear_dense(x)\n",
    "        x = self.dropout(x)\n",
    "        if inputs.shape[-1] != self.units:\n",
    "            inputs = self.project(inputs)\n",
    "        x = inputs + self.gated_linear_unit(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class VariableSelection(L.Layer):\n",
    "    def __init__(self, num_features, units, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.grns = list()\n",
    "        # Create a GRN for each feature independently\n",
    "        for idx in range(num_features):\n",
    "            grn = GatedResidualNetwork(units, dropout_rate)\n",
    "            self.grns.append(grn)\n",
    "        # Create a GRN for the concatenation of all the features\n",
    "        self.grn_concat = GatedResidualNetwork(units, dropout_rate)\n",
    "        self.softmax = L.Dense(units=num_features, activation=\"softmax\")\n",
    "        self.num_features = num_features\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['num_features'] = self.num_features\n",
    "        config['units'] = self.units\n",
    "        config['dropout_rate'] = self.dropout_rate\n",
    "        return config\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        v = L.concatenate(inputs)\n",
    "        v = self.grn_concat(v)\n",
    "        v = tf.expand_dims(self.softmax(v), axis=-1)\n",
    "\n",
    "        x = []\n",
    "        for idx, input_ in enumerate(inputs):\n",
    "            x.append(self.grns[idx](input_))\n",
    "        x = tf.stack(x, axis=1)\n",
    "\n",
    "        outputs = tf.squeeze(tf.matmul(v, x, transpose_a=True), axis=1)\n",
    "        return outputs\n",
    "    \n",
    "\n",
    "@tf.keras.utils.register_keras_serializable()\n",
    "class VariableSelectionFlow(L.Layer):\n",
    "    def __init__(self, num_features, units, dropout_rate, dense_units=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.variableselection = VariableSelection(num_features, units, dropout_rate)\n",
    "        self.split = L.Lambda(lambda t: tf.split(t, num_features, axis=-1))\n",
    "        self.dense = dense_units\n",
    "        if dense_units:\n",
    "            self.dense_list = [L.Dense(dense_units, \\\n",
    "                                       activation='linear') \\\n",
    "                               for _ in tf.range(num_features)\n",
    "                              ]\n",
    "        self.num_features = num_features\n",
    "        self.units = units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dense_units = dense_units\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config['num_features'] = self.num_features\n",
    "        config['units'] = self.units\n",
    "        config['dropout_rate'] = self.dropout_rate\n",
    "        config['dense_units'] = self.dense_units\n",
    "        return config        \n",
    "    \n",
    "    def call(self, inputs):\n",
    "        split_input = self.split(inputs)\n",
    "        if self.dense:\n",
    "            l = [self.dense_list[i](split_input[i]) for i in range(len(self.dense_list))]\n",
    "        else:\n",
    "            l = split_input\n",
    "        return self.variableselection(l)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "trainX, testX, trainy, testy = train_test_split(data.drop(columns=['target'], axis=1), data[['target']], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Models weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yukaisun/opt/anaconda3/envs/tabpfn/lib/python3.9/site-packages/sklearn/preprocessing/_label.py:116: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______fold 1______, ________repeat 1__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 81s 3s/step - loss: 1.1175 - val_loss: 1.1077 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 1.0674 - val_loss: 1.0221 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 1.0071 - val_loss: 0.9756 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.9927 - val_loss: 0.9167 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.9008 - val_loss: 0.8820 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.8975 - val_loss: 0.8429 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.8284 - val_loss: 0.7882 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.7362 - val_loss: 0.7493 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.6902 - val_loss: 0.7095 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.6310 - val_loss: 0.6610 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.6247 - val_loss: 0.6395 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.5812 - val_loss: 0.6239 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.5560 - val_loss: 0.5671 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.5367 - val_loss: 0.4921 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.5161 - val_loss: 0.4637 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4764\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.4764 - val_loss: 0.4789 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.4619\n",
      "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.4619 - val_loss: 0.4684 - lr: 9.5000e-04\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.4332 - val_loss: 0.3924 - lr: 9.0250e-04\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.4301 - val_loss: 0.3684 - lr: 9.0250e-04\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.3764\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 0.3764 - val_loss: 0.3906 - lr: 9.0250e-04\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "min_train_loss: 0.4301, min_val_loss: 0.3684, bll: 0.3906\n",
      "______fold 1______, ________repeat 2__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 81s 3s/step - loss: 1.0954 - val_loss: 1.0966 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.1347\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "3/3 [==============================] - 0s 81ms/step - loss: 1.1347 - val_loss: 1.1277 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0997\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "3/3 [==============================] - 0s 59ms/step - loss: 1.0997 - val_loss: 1.1062 - lr: 9.5000e-04\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 1.0846 - val_loss: 1.0735 - lr: 9.0250e-04\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 1.0473 - val_loss: 1.0320 - lr: 9.0250e-04\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 1.0139 - val_loss: 1.0092 - lr: 9.0250e-04\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 1.0258 - val_loss: 0.9837 - lr: 9.0250e-04\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.9440 - val_loss: 0.9477 - lr: 9.0250e-04\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 0.8853 - val_loss: 0.9066 - lr: 9.0250e-04\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.8520 - val_loss: 0.8747 - lr: 9.0250e-04\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.8274 - val_loss: 0.8387 - lr: 9.0250e-04\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.7865 - val_loss: 0.8026 - lr: 9.0250e-04\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.7650 - val_loss: 0.7679 - lr: 9.0250e-04\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.7204 - val_loss: 0.7376 - lr: 9.0250e-04\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.6801 - val_loss: 0.7091 - lr: 9.0250e-04\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 180ms/step - loss: 0.6481 - val_loss: 0.6970 - lr: 9.0250e-04\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.6238 - val_loss: 0.6541 - lr: 9.0250e-04\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.5771 - val_loss: 0.6060 - lr: 9.0250e-04\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.5791 - val_loss: 0.5531 - lr: 9.0250e-04\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.5714 - val_loss: 0.5233 - lr: 9.0250e-04\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "min_train_loss: 0.5714, min_val_loss: 0.5233, bll: 0.5233\n",
      "______fold 1______, ________repeat 3__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 78s 3s/step - loss: 1.1404 - val_loss: 1.1312 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 1.0653 - val_loss: 1.1061 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 1.0604 - val_loss: 0.9958 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.9926 - val_loss: 0.8566 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.9314 - val_loss: 0.7995 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.8539 - val_loss: 0.7458 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.7488 - val_loss: 0.7248 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.7033 - val_loss: 0.7050 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.6514 - val_loss: 0.6808 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.6080 - val_loss: 0.6562 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.5733 - val_loss: 0.6278 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.5609 - val_loss: 0.5989 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.5361 - val_loss: 0.5736 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.5244 - val_loss: 0.5734 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 0.5099 - val_loss: 0.5533 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.4876 - val_loss: 0.5087 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.4808 - val_loss: 0.4809 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.4735 - val_loss: 0.4606 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.4490 - val_loss: 0.4559 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.4479 - val_loss: 0.4140 - lr: 0.0010\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "min_train_loss: 0.4479, min_val_loss: 0.4140, bll: 0.4140\n",
      "______fold 2______, ________repeat 1__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 105s 8s/step - loss: 1.1252 - val_loss: 1.1082 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 1.1045 - val_loss: 1.0853 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 1.0572 - val_loss: 1.0387 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 1.0443 - val_loss: 0.9354 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.9868 - val_loss: 0.8079 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 169ms/step - loss: 0.8749 - val_loss: 0.7048 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.8241 - val_loss: 0.6403 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.7139 - val_loss: 0.5888 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.6963 - val_loss: 0.5503 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.6412 - val_loss: 0.5298 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.5909 - val_loss: 0.5106 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.5803 - val_loss: 0.5019 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 193ms/step - loss: 0.5422 - val_loss: 0.4966 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.5487 - val_loss: 0.4896 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.5380 - val_loss: 0.4818 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.5233 - val_loss: 0.4745 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.5120 - val_loss: 0.4647 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.5121 - val_loss: 0.4560 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.5261 - val_loss: 0.4378 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.4791 - val_loss: 0.4259 - lr: 0.0010\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1312ed0d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "min_train_loss: 0.4791, min_val_loss: 0.4259, bll: 0.4259\n",
      "______fold 2______, ________repeat 2__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 136s 4s/step - loss: 1.1345 - val_loss: 1.0084 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 1.0548 - val_loss: 0.9702 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 120ms/step - loss: 0.9905 - val_loss: 0.8234 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 0.9762 - val_loss: 0.7150 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 108ms/step - loss: 0.9116 - val_loss: 0.6757 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 103ms/step - loss: 0.8189 - val_loss: 0.6339 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.7231 - val_loss: 0.5783 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.7220 - val_loss: 0.5492 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.6623 - val_loss: 0.5373 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 104ms/step - loss: 0.6143 - val_loss: 0.5341 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.6033 - val_loss: 0.5232 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 101ms/step - loss: 0.5849 - val_loss: 0.5101 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.5658 - val_loss: 0.4985 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.5541 - val_loss: 0.4894 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 109ms/step - loss: 0.5311 - val_loss: 0.4817 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.5413 - val_loss: 0.4736 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.5217 - val_loss: 0.4662 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.5150 - val_loss: 0.4591 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.4893 - val_loss: 0.4488 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 174ms/step - loss: 0.5014 - val_loss: 0.4399 - lr: 0.0010\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x1409d5430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "min_train_loss: 0.5014, min_val_loss: 0.4399, bll: 0.4399\n",
      "______fold 2______, ________repeat 3__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 137s 4s/step - loss: 1.0972 - val_loss: 1.0393 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 176ms/step - loss: 1.0544 - val_loss: 0.9773 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 169ms/step - loss: 0.9905 - val_loss: 0.8093 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.9568 - val_loss: 0.7229 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 0.8865 - val_loss: 0.6667 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.7865 - val_loss: 0.6376 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.6986 - val_loss: 0.6096 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 146ms/step - loss: 0.7074 - val_loss: 0.5885 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 0.6573 - val_loss: 0.5696 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 215ms/step - loss: 0.6017 - val_loss: 0.5396 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 156ms/step - loss: 0.5631 - val_loss: 0.5111 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.5557 - val_loss: 0.4922 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.5195 - val_loss: 0.4804 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 204ms/step - loss: 0.5159 - val_loss: 0.4679 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 0.5149 - val_loss: 0.4605 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 153ms/step - loss: 0.5059 - val_loss: 0.4595 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.4925 - val_loss: 0.4560 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 144ms/step - loss: 0.4890 - val_loss: 0.4471 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 143ms/step - loss: 0.4748 - val_loss: 0.4358 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.4657 - val_loss: 0.4157 - lr: 0.0010\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "min_train_loss: 0.4657, min_val_loss: 0.4157, bll: 0.4157\n",
      "______fold 3______, ________repeat 1__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 124s 5s/step - loss: 1.1901 - val_loss: 1.0347 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 183ms/step - loss: 1.0938 - val_loss: 1.0078 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0885\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 1.0885 - val_loss: 1.0116 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0639\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "3/3 [==============================] - 0s 102ms/step - loss: 1.0639 - val_loss: 1.0130 - lr: 9.5000e-04\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 189ms/step - loss: 1.0489 - val_loss: 0.9977 - lr: 9.0250e-04\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 193ms/step - loss: 0.9683 - val_loss: 0.9448 - lr: 9.0250e-04\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 174ms/step - loss: 0.9422 - val_loss: 0.8646 - lr: 9.0250e-04\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 205ms/step - loss: 0.9032 - val_loss: 0.7647 - lr: 9.0250e-04\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 1s 283ms/step - loss: 0.8736 - val_loss: 0.6899 - lr: 9.0250e-04\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 0.8240 - val_loss: 0.6167 - lr: 9.0250e-04\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 0.8133 - val_loss: 0.5664 - lr: 9.0250e-04\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 198ms/step - loss: 0.7246 - val_loss: 0.5457 - lr: 9.0250e-04\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 184ms/step - loss: 0.7103 - val_loss: 0.5344 - lr: 9.0250e-04\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 186ms/step - loss: 0.6622 - val_loss: 0.5102 - lr: 9.0250e-04\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 175ms/step - loss: 0.6056 - val_loss: 0.4854 - lr: 9.0250e-04\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 215ms/step - loss: 0.5773 - val_loss: 0.4648 - lr: 9.0250e-04\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 188ms/step - loss: 0.5668 - val_loss: 0.4489 - lr: 9.0250e-04\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 1s 227ms/step - loss: 0.5716 - val_loss: 0.4359 - lr: 9.0250e-04\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 1s 373ms/step - loss: 0.5331 - val_loss: 0.4250 - lr: 9.0250e-04\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 207ms/step - loss: 0.5474 - val_loss: 0.4174 - lr: 9.0250e-04\n",
      "1/1 [==============================] - 13s 13s/step\n",
      "min_train_loss: 0.5474, min_val_loss: 0.4174, bll: 0.4174\n",
      "______fold 3______, ________repeat 2__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 170s 4s/step - loss: 1.0990 - val_loss: 1.0819 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 122ms/step - loss: 1.0450 - val_loss: 1.0799 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 1.0501 - val_loss: 0.9432 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.9933 - val_loss: 0.8686 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 0.9617 - val_loss: 0.7814 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.9079 - val_loss: 0.7563 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.8188 - val_loss: 0.6872 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 119ms/step - loss: 0.8004 - val_loss: 0.6284 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 0.7293 - val_loss: 0.6036 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 0.6716 - val_loss: 0.5664 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 0.6338 - val_loss: 0.5338 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.6151 - val_loss: 0.4928 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 0.5966 - val_loss: 0.4518 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 1s 396ms/step - loss: 0.5640 - val_loss: 0.4249 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 157ms/step - loss: 0.5561 - val_loss: 0.4144 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.5515 - val_loss: 0.4133 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 163ms/step - loss: 0.5351 - val_loss: 0.3999 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 199ms/step - loss: 0.5377 - val_loss: 0.3598 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5225\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.5225 - val_loss: 0.3631 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 184ms/step - loss: 0.5004 - val_loss: 0.3517 - lr: 9.5000e-04\n",
      "1/1 [==============================] - 7s 7s/step\n",
      "min_train_loss: 0.5004, min_val_loss: 0.3517, bll: 0.3517\n",
      "______fold 3______, ________repeat 3__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 99s 3s/step - loss: 1.1742 - val_loss: 1.0839 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0796\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "3/3 [==============================] - 0s 54ms/step - loss: 1.0796 - val_loss: 1.0940 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 135ms/step - loss: 1.0761 - val_loss: 1.0101 - lr: 9.5000e-04\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 1.0279 - val_loss: 0.8758 - lr: 9.5000e-04\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9877 - val_loss: 0.7975 - lr: 9.5000e-04\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 115ms/step - loss: 0.9518 - val_loss: 0.7262 - lr: 9.5000e-04\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.8273 - val_loss: 0.6619 - lr: 9.5000e-04\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.8260 - val_loss: 0.6200 - lr: 9.5000e-04\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.7384 - val_loss: 0.5826 - lr: 9.5000e-04\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.7262 - val_loss: 0.5563 - lr: 9.5000e-04\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.6688 - val_loss: 0.5271 - lr: 9.5000e-04\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.6587 - val_loss: 0.4900 - lr: 9.5000e-04\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 0.6033 - val_loss: 0.4589 - lr: 9.5000e-04\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.5573 - val_loss: 0.4341 - lr: 9.5000e-04\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.5675 - val_loss: 0.4154 - lr: 9.5000e-04\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.5379 - val_loss: 0.4024 - lr: 9.5000e-04\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.5313 - val_loss: 0.3868 - lr: 9.5000e-04\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.5134 - val_loss: 0.3728 - lr: 9.5000e-04\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.5024 - val_loss: 0.3609 - lr: 9.5000e-04\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.4947 - val_loss: 0.3522 - lr: 9.5000e-04\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "min_train_loss: 0.4947, min_val_loss: 0.3522, bll: 0.3522\n",
      "______fold 4______, ________repeat 1__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 111s 5s/step - loss: 1.0849 - val_loss: 1.0914 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 157ms/step - loss: 1.0577 - val_loss: 1.0509 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 1.0487 - val_loss: 0.7476 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 1s 265ms/step - loss: 0.9340 - val_loss: 0.7455 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 196ms/step - loss: 0.9860 - val_loss: 0.7202 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 1s 274ms/step - loss: 0.8036 - val_loss: 0.6858 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 105ms/step - loss: 0.8404 - val_loss: 0.6736 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 176ms/step - loss: 0.8336 - val_loss: 0.6720 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 178ms/step - loss: 0.7386 - val_loss: 0.6623 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 1s 270ms/step - loss: 0.7235 - val_loss: 0.6437 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 127ms/step - loss: 0.7269 - val_loss: 0.6231 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 0.6716 - val_loss: 0.6046 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 1s 282ms/step - loss: 0.6266 - val_loss: 0.5865 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.6157 - val_loss: 0.5719 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.5839 - val_loss: 0.5555 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 0.5891 - val_loss: 0.5424 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 163ms/step - loss: 0.5714 - val_loss: 0.5326 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.5707 - val_loss: 0.5269 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 118ms/step - loss: 0.5444 - val_loss: 0.5202 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 112ms/step - loss: 0.5190 - val_loss: 0.5121 - lr: 0.0010\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "min_train_loss: 0.5190, min_val_loss: 0.5121, bll: 0.5121\n",
      "______fold 4______, ________repeat 2__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 144s 6s/step - loss: 1.1109 - val_loss: 1.0945 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 1s 239ms/step - loss: 1.0797 - val_loss: 1.0676 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 132ms/step - loss: 1.0492 - val_loss: 0.9964 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 125ms/step - loss: 1.0442 - val_loss: 0.9780 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 124ms/step - loss: 1.0006 - val_loss: 0.9086 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 162ms/step - loss: 0.9880 - val_loss: 0.9075 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 1s 292ms/step - loss: 1.0043 - val_loss: 0.8420 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 1s 245ms/step - loss: 0.9749 - val_loss: 0.8113 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 1s 287ms/step - loss: 0.8963 - val_loss: 0.8095 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 156ms/step - loss: 0.8701 - val_loss: 0.7866 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 123ms/step - loss: 0.8156 - val_loss: 0.7649 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 139ms/step - loss: 0.7866 - val_loss: 0.7446 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 210ms/step - loss: 0.7766 - val_loss: 0.7238 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 183ms/step - loss: 0.7325 - val_loss: 0.7003 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 138ms/step - loss: 0.6992 - val_loss: 0.6769 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 0.6810 - val_loss: 0.6420 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 126ms/step - loss: 0.6565 - val_loss: 0.5984 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 169ms/step - loss: 0.6288 - val_loss: 0.5634 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 211ms/step - loss: 0.6095 - val_loss: 0.5387 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 219ms/step - loss: 0.5836 - val_loss: 0.5063 - lr: 0.0010\n",
      "1/1 [==============================] - 9s 9s/step\n",
      "min_train_loss: 0.5836, min_val_loss: 0.5063, bll: 0.5063\n",
      "______fold 4______, ________repeat 3__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 151s 4s/step - loss: 1.1082 - val_loss: 1.1217 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 1s 259ms/step - loss: 1.0932 - val_loss: 1.1131 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 1s 234ms/step - loss: 1.1020 - val_loss: 1.0918 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 212ms/step - loss: 1.0868 - val_loss: 1.0777 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 159ms/step - loss: 1.0804 - val_loss: 1.0620 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 1s 267ms/step - loss: 1.0714 - val_loss: 1.0271 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 1s 267ms/step - loss: 1.0547 - val_loss: 0.9894 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 1s 367ms/step - loss: 1.0429 - val_loss: 0.9554 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 1s 275ms/step - loss: 1.0267 - val_loss: 0.9115 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 0.9838 - val_loss: 0.8838 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 160ms/step - loss: 0.9388 - val_loss: 0.8515 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 187ms/step - loss: 0.9075 - val_loss: 0.8202 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.8600 - val_loss: 0.7980 - lr: 0.0010\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.8282 - val_loss: 0.7814 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 0.7742 - val_loss: 0.7598 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.7426 - val_loss: 0.7241 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.7032 - val_loss: 0.6799 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 110ms/step - loss: 0.6932 - val_loss: 0.6508 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 114ms/step - loss: 0.6410 - val_loss: 0.6187 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 222ms/step - loss: 0.6645 - val_loss: 0.5627 - lr: 0.0010\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "min_train_loss: 0.6645, min_val_loss: 0.5627, bll: 0.5627\n",
      "______fold 5______, ________repeat 1__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 109s 3s/step - loss: 1.1460 - val_loss: 0.9473 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 199ms/step - loss: 1.0343 - val_loss: 0.8680 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 134ms/step - loss: 1.0195 - val_loss: 0.8674 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9919\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 0.9919 - val_loss: 0.9999 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.9167\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "3/3 [==============================] - 0s 52ms/step - loss: 0.9167 - val_loss: 0.8886 - lr: 9.5000e-04\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 121ms/step - loss: 0.9233 - val_loss: 0.8189 - lr: 9.0250e-04\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.8502 - val_loss: 0.7774 - lr: 9.0250e-04\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.8017 - val_loss: 0.7520 - lr: 9.0250e-04\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.7396 - val_loss: 0.7345 - lr: 9.0250e-04\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.7114 - val_loss: 0.7055 - lr: 9.0250e-04\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 116ms/step - loss: 0.6249 - val_loss: 0.6901 - lr: 9.0250e-04\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.6247 - val_loss: 0.6866 - lr: 9.0250e-04\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.5748 - val_loss: 0.6750 - lr: 9.0250e-04\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 90ms/step - loss: 0.5846 - val_loss: 0.6674 - lr: 9.0250e-04\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.5547 - val_loss: 0.6463 - lr: 9.0250e-04\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.5526 - val_loss: 0.6152 - lr: 9.0250e-04\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.5286 - val_loss: 0.5909 - lr: 9.0250e-04\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.5227 - val_loss: 0.5752 - lr: 9.0250e-04\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.4981 - val_loss: 0.5636 - lr: 9.0250e-04\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.4947 - val_loss: 0.5510 - lr: 9.0250e-04\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "min_train_loss: 0.4947, min_val_loss: 0.5510, bll: 0.5510\n",
      "______fold 5______, ________repeat 2__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 79s 3s/step - loss: 1.1139 - val_loss: 1.0319 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0679\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 1.0679 - val_loss: 1.0926 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0592\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "3/3 [==============================] - 0s 56ms/step - loss: 1.0592 - val_loss: 1.0971 - lr: 9.5000e-04\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 1.0507\n",
      "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.0008573750033974647.\n",
      "3/3 [==============================] - 0s 53ms/step - loss: 1.0507 - val_loss: 1.0407 - lr: 9.0250e-04\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 1.0150 - val_loss: 0.9542 - lr: 8.5737e-04\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9659 - val_loss: 0.8803 - lr: 8.5737e-04\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 96ms/step - loss: 0.9669 - val_loss: 0.8511 - lr: 8.5737e-04\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.9160 - val_loss: 0.8228 - lr: 8.5737e-04\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 113ms/step - loss: 0.8613 - val_loss: 0.8148 - lr: 8.5737e-04\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.8219 - val_loss: 0.7982 - lr: 8.5737e-04\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.7641 - val_loss: 0.7762 - lr: 8.5737e-04\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.7119 - val_loss: 0.7463 - lr: 8.5737e-04\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 111ms/step - loss: 0.7080 - val_loss: 0.7196 - lr: 8.5737e-04\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.6598 - val_loss: 0.6936 - lr: 8.5737e-04\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 167ms/step - loss: 0.6470 - val_loss: 0.6606 - lr: 8.5737e-04\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.6327 - val_loss: 0.6350 - lr: 8.5737e-04\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 0.5936 - val_loss: 0.6257 - lr: 8.5737e-04\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 117ms/step - loss: 0.5783 - val_loss: 0.5992 - lr: 8.5737e-04\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.5550 - val_loss: 0.5667 - lr: 8.5737e-04\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - 0s 95ms/step - loss: 0.5512 - val_loss: 0.5495 - lr: 8.5737e-04\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "min_train_loss: 0.5512, min_val_loss: 0.5495, bll: 0.5495\n",
      "______fold 5______, ________repeat 3__________\n",
      "Epoch 1/20\n",
      "3/3 [==============================] - 81s 3s/step - loss: 1.0845 - val_loss: 1.0039 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "3/3 [==============================] - 0s 98ms/step - loss: 1.0037 - val_loss: 0.9695 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.9504 - val_loss: 0.9370 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.9474 - val_loss: 0.8821 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.8778 - val_loss: 0.8509 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "3/3 [==============================] - 0s 107ms/step - loss: 0.8269 - val_loss: 0.8259 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "3/3 [==============================] - 0s 100ms/step - loss: 0.7861 - val_loss: 0.7906 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.7861 - val_loss: 0.7745 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.8013 - val_loss: 0.7711 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.6967\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.0009500000451225787.\n",
      "3/3 [==============================] - 0s 74ms/step - loss: 0.6967 - val_loss: 0.7732 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "3/3 [==============================] - 0s 91ms/step - loss: 0.6681 - val_loss: 0.7631 - lr: 9.5000e-04\n",
      "Epoch 12/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.6528 - val_loss: 0.7450 - lr: 9.5000e-04\n",
      "Epoch 13/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.6490 - val_loss: 0.7311 - lr: 9.5000e-04\n",
      "Epoch 14/20\n",
      "3/3 [==============================] - 0s 97ms/step - loss: 0.6192 - val_loss: 0.7092 - lr: 9.5000e-04\n",
      "Epoch 15/20\n",
      "3/3 [==============================] - 0s 93ms/step - loss: 0.5913 - val_loss: 0.6863 - lr: 9.5000e-04\n",
      "Epoch 16/20\n",
      "3/3 [==============================] - 0s 92ms/step - loss: 0.5748 - val_loss: 0.6592 - lr: 9.5000e-04\n",
      "Epoch 17/20\n",
      "3/3 [==============================] - 0s 99ms/step - loss: 0.5573 - val_loss: 0.6335 - lr: 9.5000e-04\n",
      "Epoch 18/20\n",
      "3/3 [==============================] - 0s 140ms/step - loss: 0.5411 - val_loss: 0.6160 - lr: 9.5000e-04\n",
      "Epoch 19/20\n",
      "3/3 [==============================] - 0s 94ms/step - loss: 0.5182 - val_loss: 0.6027 - lr: 9.5000e-04\n",
      "Epoch 20/20\n",
      "3/3 [==============================] - ETA: 0s - loss: 0.5281\n",
      "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0009025000152178108.\n",
      "3/3 [==============================] - 0s 55ms/step - loss: 0.5281 - val_loss: 0.6117 - lr: 9.5000e-04\n",
      "1/1 [==============================] - 5s 5s/step\n",
      "min_train_loss: 0.5182, min_val_loss: 0.6027, bll: 0.6117\n",
      "0.4682737\n",
      "CPU times: user 27min 24s, sys: 1min 12s, total: 28min 36s\n",
      "Wall time: 32min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import optimizers as O\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "blls = []\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "units_1 = 32\n",
    "drop_1 = 0.75\n",
    "dense_units = 8\n",
    "\n",
    "units_2 = 16\n",
    "drop_2 = 0.5\n",
    "\n",
    "units_3 = 8\n",
    "drop_3 = 0.25\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)#StratifiedKFold(n_splits=5, shuffle=True, random_state=722)\n",
    "X = trainX.reset_index(drop=True).values\n",
    "y = trainy.reset_index(drop=True).values\n",
    "y_enc = LabelEncoder().fit_transform(y)\n",
    "y_label = tf.keras.utils.to_categorical(y_enc)\n",
    "for n, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    for k in range(3):\n",
    "        print(f'______fold {n+1}______, ________repeat {k+1}__________')\n",
    "        inputs_1 = tf.keras.Input(shape=(trainX.shape[1],))\n",
    "        \n",
    "        features_1 = VariableSelectionFlow(trainX.shape[1], units_1, drop_1, dense_units=dense_units)(inputs_1)\n",
    "        features_2 = VariableSelectionFlow(units_1, units_2, drop_2)(features_1)         \n",
    "        features_3 = VariableSelectionFlow(units_2, units_3, drop_3)(features_2)         \n",
    "\n",
    "        outputs = L.Dense(3, activation=\"softmax\")(features_3)\n",
    "\n",
    "        model = Model(inputs=inputs_1, outputs=outputs)      \n",
    "\n",
    "        opt = O.Adam(1e-3, epsilon=1e-7)\n",
    "        loss = CategoricalCrossentropy()\n",
    "\n",
    "        lr = ReduceLROnPlateau(monitor=\"val_loss\", mode='min', factor=0.95, patience=1, verbose=1)\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', patience=25, verbose=1, restore_best_weights=True)\n",
    "\n",
    "        model.compile(optimizer=opt, loss=loss)\n",
    "        history = model.fit(x=X[train_idx], y=y_label[train_idx],\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=20,\n",
    "                          validation_data=(X[val_idx], y_label[val_idx]),\n",
    "                          callbacks=[lr, es])\n",
    "                \n",
    "        #probs = model.predict(X[val_idx])[:,0]\n",
    "        bll = loss(y_label[val_idx], model.predict(X[val_idx]))\n",
    "        blls.append(bll)\n",
    "        val_loss = np.asarray(history.history['val_loss'])\n",
    "        train_loss = np.asarray(history.history['loss'])\n",
    "        min_val_loss = val_loss.min()\n",
    "        min_train_loss = train_loss[val_loss.argmin()]\n",
    "        print(f'min_train_loss: {min_train_loss:.4f}, min_val_loss: {min_val_loss:.4f}, bll: {bll:.4f}')  \n",
    "        \n",
    "        model.save_weights(f'models_weights/mod_f{n}_r{k}.h5')\n",
    "        \n",
    "print(np.mean(blls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "mfolder = 'models_weights/'\n",
    "models_weights = os.listdir(mfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 36s 31ms/step\n",
      "2/2 [==============================] - 21s 19ms/step\n",
      "2/2 [==============================] - 19s 31ms/step\n",
      "2/2 [==============================] - 12s 15ms/step\n",
      "2/2 [==============================] - 10s 14ms/step\n",
      "2/2 [==============================] - 10s 33ms/step\n",
      "2/2 [==============================] - 10s 27ms/step\n",
      "2/2 [==============================] - 10s 27ms/step\n",
      "2/2 [==============================] - 10s 21ms/step\n",
      "2/2 [==============================] - 12s 24ms/step\n",
      "2/2 [==============================] - 10s 22ms/step\n",
      "2/2 [==============================] - 12s 15ms/step\n",
      "2/2 [==============================] - 8s 15ms/step\n",
      "2/2 [==============================] - 17s 14ms/step\n",
      "2/2 [==============================] - 14s 16ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.zeros_like(testy.iloc[:,0].values, dtype=np.float32)\n",
    "batch_size = 32\n",
    "\n",
    "units_1 = 32\n",
    "drop_1 = 0.75\n",
    "dense_units = 8\n",
    "\n",
    "units_2 = 16\n",
    "drop_2 = 0.5\n",
    "\n",
    "units_3 = 8\n",
    "drop_3 = 0.25\n",
    "\n",
    "predys = [[0]*len(testX) for _ in range(len(models_weights))]\n",
    "predyss = [[0]*len(testX) for _ in range(len(models_weights))]\n",
    "for n, model_weights in enumerate(models_weights):\n",
    "    inputs_1 = tf.keras.Input(shape=(4,))\n",
    "    \n",
    "    features_1 = VariableSelectionFlow(4, units_1, drop_1, dense_units=dense_units)(inputs_1)\n",
    "    features_2 = VariableSelectionFlow(units_1, units_2, drop_2)(features_1)\n",
    "    features_3 = VariableSelectionFlow(units_2, units_3, drop_3)(features_2)\n",
    "\n",
    "    outputs = L.Dense(3, activation=\"softmax\")(features_3)\n",
    "\n",
    "    model = Model(inputs=inputs_1, outputs=outputs)\n",
    "    model.load_weights(mfolder + model_weights)\n",
    "    predy_ = model.predict(testX)\n",
    "    predys[n] = np.argmax(predy_, axis=1)\n",
    "    predyss[n] = predy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predy=[]\n",
    "for j in range(len(predys[0])):\n",
    "    lst = [predys[i][j] for i in range(len(predys))]\n",
    "    predy.append(max(set(lst), key=lst.count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 97.37 %\n"
     ]
    }
   ],
   "source": [
    "print(':',format(sum(predy==testy.values.ravel())*100/len(testy), '.2f'),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "predy = np.argmax(np.divide((sum(predyss)/15).T, np.sum(sum(predyss)/15, axis=1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 97.37 %\n"
     ]
    }
   ],
   "source": [
    "print(':',format(sum(predy==testy.values.ravel())*100/len(testy), '.2f'),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
